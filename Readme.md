#  Automated Resolution of Web Performance Issues Using LLMs: A Case Study of GPT-4o-mini

Abstract:

Users demand fast, seamless experiences when using webpages, while developers face the challenge of meeting these expectations within tight constraints. Creating dynamic and reliable applications often involves balancing innovation with performance optimization—a task that is both critical and time-consuming.
Performance engineering addresses this challenge, helping developers optimize webpages for speed, responsiveness, and reliability. However, the process often requires manual effort to analyze performance audits and implement fixes, diverting time from other important tasks. Recent advances in AI, particularly in Large Language Models (LLMs), have shown promise in automating complex tasks that involve natural language understanding and structured data processing. Leveraging LLMs for web performance optimization could transform how developers address performance issues, reducing effort and accelerating feature delivery. This study evaluates GPT-4o-mini’s effectiveness in automating the resolution of web performance issues. For this purpose, we first extracted the Document Object Model (DOM) trees of 15 popular webpages (e.g., Facebook), and then we used Lighthouse to generate performance audits. Subsequently, we passed the extracted DOM trees and their corresponding audits through GPT-4o-mini for resolution. Our study uncovers 67 unique audits which we categorize into seven types. By comparing the audits of the original webpages and that of the GPT-modified webpages, our results show that GPT-4o-mini resolved all SEO or Accessibility related audit issues. We also observed a 14\% decrease in runtime performance issues. However, GPT-4o-mini significantly increased five audit types, with the majority of issues being hallucinated changes. Our study demonstrates the potential of LLMs like GPT-4o-mini to facilitate web development by automating critical performance optimization tasks, offering insights into the feasibility of automating this traditionally manual process. However, it also emphasizes the need for supervision of these tasks. 

## Directories

### Dataset

The `dataset` directory contains the following:

- **Originally Extracted DOMs**: The original DOM trees extracted from the webpages.
- **DOM Chunks**: Smaller chunks of the DOM trees, used for processing by GPT-4o-mini.
- **Lighthouse Reports of Original DOMs**: Performance audits generated by Lighthouse for the original DOM trees.
- **Lighthouse Reports of Modified DOMs**: Performance audits generated by Lighthouse for the modified DOM trees.
- **Prompts and Response Times**: The prompts used to interact with GPT-4o-mini and the response times for each experiment.

### Scripts

The `scripts` directory contains the following Python scripts:

- **dom_extractor.py**: Extracts the DOM tree from a webpage.
- **dom_analyser.py**: Analyzes the DOM tree for its attributes and hierarchical information.
- **html_mapper.py**: Maps the structure of the HTML content.
- **html_chunker.py**: Breaks down the DOM tree into smaller chunks for processing.
- **html_distance_calculator.py**: Calculates the differences between the original and modified DOM trees.
- **html_modifier.py**: Modifies the DOM tree based on GPT-4o-mini's recommendations.
- **html_pipeline.py**: Orchestrates the entire process of DOM extraction, analysis, and modification.
- **report_generator.py**: Generates performance reports using Lighthouse.
- **requirements.txt**: Lists the Python dependencies required for the project.
- **Dockerfile**: Defines the Docker image for the project.
- **docker-compose.yml**: Configures the Docker containers for the project.
- **entry-point.sh**: The entry point script for the Docker container.

The `Additional-documentation.pdf` contains our prompt design template explicitly listed out as well as audits and their definitions as used in our study.

## Getting Started

To get started with the project, follow these steps:

1. Install the required Python dependencies:
   ```sh
   pip install -r requirements.txt

2. Run the desired Python script:
    ```sh
    python script_name.py

## Audit Generation using Lighthouse

To generate performance audits using Lighthouse, run the following command:

```sh
docker run --shm-size=1g -dit -p 8000:8000 report-generator